{"componentChunkName":"component---src-templates-blog-post-js","path":"/Linear Regression/","result":{"data":{"site":{"siteMetadata":{"title":"ljho01"}},"markdownRemark":{"id":"7c12712a-f309-5729-b387-732938c71634","excerpt":"선형 회귀 회귀분석 중 가장 기본적인 형태. 선형 상관 관계()를 찾는다.\r\n기울기 a를 가중치, b를 보정치 혹은 편향이라고 부른다.\r\n독립변수(x…","html":"<h1 id=\"선형-회귀\" style=\"position:relative;\"><a href=\"#%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80\" aria-label=\"선형 회귀 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>선형 회귀</h1>\n<p>회귀분석 중 가장 기본적인 형태. 선형 상관 관계(<code class=\"language-text\">ax + b</code>)를 찾는다.\r\n기울기 a를 가중치, b를 보정치 혹은 편향이라고 부른다.\r\n독립변수(x)가 한개일경우 단순 회귀분석, 여러개일 경우 다중 회귀분석이라고 한다. 여기서 선형회귀를 사용하면 선이 면이 되겠지?(<code class=\"language-text\">ax+by+cz+...+C</code>) 이것이 선형 결합(Linear Combination)이다. 직선식을 여러개 합친 것이라고 생각하자.</p>\n<p>a, b, c 등의 파라미터들을 정할 때 그 값에 대한 평가를 해야한다. 그럴 때 비용 함수를 사용한다.</p>\n<h4 id=\"비용-함수cost-function\" style=\"position:relative;\"><a href=\"#%EB%B9%84%EC%9A%A9-%ED%95%A8%EC%88%98cost-function\" aria-label=\"비용 함수cost function permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>비용 함수(cost function)</h4>\n<p>cost 대신 loss, error, objective(목적 함수는 문제를 해결하기 위해 값을 줄이거나 늘려야하는 함수를 말한다. 여기선 cost함수를 최소화시켜야 한다.)가 쓰이기도 한다.</p>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/dc759aed-370a-402e-91de-6bfd6f0e0f35/image.png\" alt=\"https://www.kaggle.com/general/156593\"></p>\n<p>Mean Squared Error Function(MSE)라는 가장 흔한 비용함수이다. 보통 비용 함수는 <code class=\"language-text\">J</code>로 표시한다.\r\n오차를 제곱하여 평균을 내어주면 된다. 앞에 1/2를 곱하는 것도 있고 안 곱하는 것도 있는 것 같다.\r\n변종으로 Mean Absolute Error Func(MAE)라는 것도 있다고 한다. 제곱이 들어간 MSE는\r\nMAE보다 아웃라이어에 민감하게 변한다.\r\n또 Mean Absolute Percentage Error(MAPE)라고 절대오차가 아닌 상대오차에 절댓값을 씌워 평균을 낸 것도 있다.\r\n<img src=\"https://velog.velcdn.com/images/ljho01/post/a1271c52-c01e-4850-a477-c5096e30ceeb/image.png\" alt=\"https://towardsdatascience.com/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac\">\r\nMSE에 루트를 씌운 RMSE라는 것도 있다.</p>\n<h2 id=\"경사-하강법gradient-decent-algorithm\" style=\"position:relative;\"><a href=\"#%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95gradient-decent-algorithm\" aria-label=\"경사 하강법gradient decent algorithm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>경사 하강법(Gradient Decent Algorithm)</h2>\n<p>그럼 목적함수를 어떻게 해결할까?</p>\n<p>비용 함수는 어찌됬건 대부분의 점에서 미분가능한 그래프를 만들 것이다. 사람은 그래프를 대충 그려보고 최솟값을 구할 수 있겠지만 컴퓨터는 그래프를 그리려면 무한개의 점을 찍어야한다. 그래서 미분 후 최소값을 찾는 방식으로 최솟값을 찾는다.</p>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/7cd7835b-f8e3-487a-83a0-614a1750223c/image.png\" alt=\"https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/\"></p>\n<p>랜덤한 지점을 미분 후 쭉 내려가는 방식으로 J' = 0의 해를 찾아 해결한다. (미분 방정식을 직접 풀어서 사용할 수도 있다.)</p>\n<p><strong>근데 위와 같은 방식을 사용했다가 잘못해서 Local Minima들로 빠져서 Global Minimum을 놓친다면?</strong></p>\n<p>빠져나오는 방식이 몇개 있다고 하는데 보통 Global Minimum과 비슷한 값으로 떨어진다고 한다.</p>\n<p>보폭을 나타내는 알파(learning rate, step size)는 수동으로 설정하는 상수다. 너무 작으면 속도가 느려지고, 너무 크면 왔다갔다해서 정확도가 떨어질 수 있다. 일반적으로 0.01 ~ 0.001 사이에서 잡는다. 이와 같이 사람이 결정하는 변수를 <code class=\"language-text\">Hyper-parameter</code>라고 한다. 이를 조절하는 것을 Model Tuning이라고 한다.</p>\n<p>번외로 AutoML이라는 것이 있는데, 이는 데이터 전처리, 모델 선택, 하이퍼 파라미터 최적화(HPO)를 알아서 해주는 것이다.</p>\n<h1 id=\"scikit-learn\" style=\"position:relative;\"><a href=\"#scikit-learn\" aria-label=\"scikit learn permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>scikit-learn</h1>\n<p>파이썬으로 traditional ML 알고리즘들을 구현한 오픈 소스 라이브러리.\r\n장점</p>\n<ul>\n<li>타 라이브러리들과 호환성이 좋음 (Numpy, Pandas 등)</li>\n<li>통일된 인터페이스</li>\n</ul>\n<h2 id=\"사용법\" style=\"position:relative;\"><a href=\"#%EC%82%AC%EC%9A%A9%EB%B2%95\" aria-label=\"사용법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>사용법</h2>\n<h3 id=\"1-데이터-불러오기\" style=\"position:relative;\"><a href=\"#1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0\" aria-label=\"1 데이터 불러오기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 데이터 불러오기</h3>\n<p><code class=\"language-text\">sklearn.datasets.load_[DATA]()</code> 함수를 통해 데이터를 불러온다.\r\n데이터의 형식은 <code class=\"language-text\">numpy.array</code>형식으로 주면 된다.</p>\n<p>유명한 데이터는 내장되어있다. 예를 들면\r\n<code class=\"language-text\">var = sklearn.datasets.load_iris()</code> 이렇게 불러올 수 있다.\r\nx를 불러오려면\r\n<code class=\"language-text\">var.data</code>\r\ny를 불러오려면\r\n<code class=\"language-text\">var.target</code> 이렇게</p>\n<h3 id=\"2-데이터-쪼개기\" style=\"position:relative;\"><a href=\"#2-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%AA%BC%EA%B0%9C%EA%B8%B0\" aria-label=\"2 데이터 쪼개기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 데이터 쪼개기</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size)</code></pre></div>\n<p><code class=\"language-text\">test_size</code>를 0.3으로 설정하면 0.7을 학습 데이터, 나머지를 테스트 데이터로 사용한다. validation data는 이 함수에서는 없다.</p>\n<h3 id=\"3-모델-객체-생성\" style=\"position:relative;\"><a href=\"#3-%EB%AA%A8%EB%8D%B8-%EA%B0%9D%EC%B2%B4-%EC%83%9D%EC%84%B1\" aria-label=\"3 모델 객체 생성 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 모델 객체 생성</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">model = sklearn.linear_model.~~~()</code></pre></div>\n<p>Hyperparameter가 있는 모델들은 argument로 넣어주면 된다.\r\n이렇게 모델을 생성하고</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">model.fit(train_x, train_y)</code></pre></div>\n<p>학습을 시켜주면 된다.</p>\n<p>이제 예측을 하고 Cost function을 돌려보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">pred_y = model.predict(test_x)\r\nsklearn.metrics.mean_squared_error(pred_y, test_y)</code></pre></div>\n<p><code class=\"language-text\">mean_squared_error</code> 외에도 성능지표로 <code class=\"language-text\">accuracy_score</code>(분류문제만), <code class=\"language-text\">precision_score</code>, <code class=\"language-text\">recall_score</code>, <code class=\"language-text\">r2_score</code> 등등 많다.</p>\n<h1 id=\"데이터-정제\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EC%A0%9C\" aria-label=\"데이터 정제 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터 정제</h1>\n<p>numerical -> <strong>min-max algorithm</strong> or <strong>standardization</strong>\r\ncategorical -> <strong>one-hot encoding</strong></p>\n<h2 id=\"standardization\" style=\"position:relative;\"><a href=\"#standardization\" aria-label=\"standardization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Standardization</h2>\n<p>표준화이다. 고등학교 때 하던 (x-표준편차)/평균 이다.</p>\n<h2 id=\"one-hot-encoding\" style=\"position:relative;\"><a href=\"#one-hot-encoding\" aria-label=\"one hot encoding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>One-Hot Encoding</h2>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/e4b0a5c3-5719-4fef-b3c9-b81bbba79786/image.png\" alt=\"https://www.brainstobytes.com/one-hot-encoding-with-pokemon/\">\r\ncategorical column과 numerical column을 명확히 구분짓기 위해 categorical column은 정수가 아닌 벡터로 변환해서 사용한다.</p>\n<p>Linear 기반 모델들은 무조건 해줘야 하는 작업.\r\nTree 계열 모델들은 굳이 해줄 필요가 없다고 한다.</p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#%EC%84%A0%ED%98%95-%ED%9A%8C%EA%B7%80\">선형 회귀</a></p>\n<ul>\n<li>\n<ul>\n<li>\n<ul>\n<li><a href=\"#%EB%B9%84%EC%9A%A9-%ED%95%A8%EC%88%98cost-function\">비용 함수(cost function)</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EA%B2%BD%EC%82%AC-%ED%95%98%EA%B0%95%EB%B2%95gradient-decent-algorithm\">경사 하강법(Gradient Decent Algorithm)</a></p>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#scikit-learn\">scikit-learn</a></p>\n<ul>\n<li>\n<p><a href=\"#%EC%82%AC%EC%9A%A9%EB%B2%95\">사용법</a></p>\n<ul>\n<li><a href=\"#1-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0\">1. 데이터 불러오기</a></li>\n<li><a href=\"#2-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%AA%BC%EA%B0%9C%EA%B8%B0\">2. 데이터 쪼개기</a></li>\n<li><a href=\"#3-%EB%AA%A8%EB%8D%B8-%EA%B0%9D%EC%B2%B4-%EC%83%9D%EC%84%B1\">3. 모델 객체 생성</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%95%EC%A0%9C\">데이터 정제</a></p>\n<ul>\n<li><a href=\"#standardization\">Standardization</a></li>\n<li><a href=\"#one-hot-encoding\">One-Hot Encoding</a></li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"AI/ML/Linear Regression","date":"October 31, 2022","description":null}},"previous":{"fields":{"slug":"/AI개요/"},"frontmatter":{"title":"AI/개요"}},"next":{"fields":{"slug":"/LogisticRegression/"},"frontmatter":{"title":"AI/ML/Logistic Regression"}}},"pageContext":{"id":"7c12712a-f309-5729-b387-732938c71634","previousPostId":"8be972e6-07a4-56e9-ad9e-e90b182e1a4f","nextPostId":"bd39c1b4-92fc-578d-b3e9-92f47200b86f"}},"staticQueryHashes":["2841359383"],"slicesMap":{}}