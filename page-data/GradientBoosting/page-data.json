{"componentChunkName":"component---src-templates-blog-post-js","path":"/GradientBoosting/","result":{"data":{"site":{"siteMetadata":{"title":"ljho01"}},"markdownRemark":{"id":"211b08e7-7504-5979-82c7-7585c5feb82e","excerpt":"Gradient Boosting Regression 실습(회귀) https://scikit-learn.org/0.20/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble…","html":"<h1 id=\"gradient-boosting-regression-실습회귀\" style=\"position:relative;\"><a href=\"#gradient-boosting-regression-%EC%8B%A4%EC%8A%B5%ED%9A%8C%EA%B7%80\" aria-label=\"gradient boosting regression 실습회귀 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boosting Regression 실습(회귀)</h1>\n<p><a href=\"https://scikit-learn.org/0.20/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\">https://scikit-learn.org/0.20/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py</a></p>\n<p>위 링크를 참고함</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn import datasets, ensemble\r\nfrom sklearn.utils import shuffle\r\nfrom sklearn.metrics import mean_squared_error</code></pre></div>\n<p>boosting기법을 사용하므로 ensemble을 불러와준다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Load data\r\nboston = datasets.load_boston()\r\nX, y = shuffle(boston.data, boston.target, random_state=13)\r\nX = X.astype(np.float32)\r\noffset = int(X.shape[0] * 0.9)\r\nX_train, y_train = X[:offset], y[:offset]\r\nX_test, y_test = X[offset:], y[offset:]</code></pre></div>\n<p>shuffle 함수로 데이터를 섞고 astype으로 전부 np.float32로 바꿔준다.\r\n그 다음은 train / test를 9:1로 쪼개는 모습. 굳이 이러지 말고<code class=\"language-text\">model_selection.train_test_split(x, y, ratio)</code> 를 쓰자.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Fit regression model\r\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\r\n          'learning_rate': 0.01, 'loss': 'ls'}\r\nclf = ensemble.GradientBoostingRegressor(**params)\r\n\r\nclf.fit(X_train, y_train)\r\nmse = mean_squared_error(y_test, clf.predict(X_test))\r\nprint(\"MSE: %.4f\" % mse)</code></pre></div>\n<p>prams에 hyper-parameter를 정리해 넣어준다. 근데 <code class=\"language-text\">**params</code> 이게 낯설다. argument를 이런식으로도 넣을 수 있구나\r\nn_estimators: 의사결정나무(weak learner)의 개수\r\nmax_depth: 의사결정나무의 최대깊이\r\nmin_samples_split: 의사결정나무로 쪼갰을때 leaf node의 최소 데이터량\r\nlearning_rate: gradient decent 적용시 알파의 값. 작을수록 세밀하게 이동하며 계산량이 많아지고, 너무크면 답이 안나온다. 일반적으로 0.01~0.001사이에서 설정한다.\r\n<img src=\"https://velog.velcdn.com/images/ljho01/post/5b9d1dee-4a41-4bb2-a2bb-8d285e8ca72c/image.png\" alt=\"https://www.jeremyjordan.me/nn-learning-rate/\">\r\nloss: cost function의 종류를 설정하는것. 위에서 적용한 ls는 최소자승법으로 mse랑 같다.</p>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/03fa1455-96af-486a-ad73-319be2054437/image.png\" alt=\"\"></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Plot training deviance\r\n\r\n# compute test set deviance\r\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\r\n\r\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\r\n    test_score[i] = clf.loss_(y_test, y_pred)\r\n\r\nplt.figure(figsize=(12, 6))\r\nplt.subplot(1, 2, 1)\r\nplt.title('Deviance')\r\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\r\n         label='Training Set Deviance')\r\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\r\n         label='Test Set Deviance')\r\nplt.legend(loc='upper right')\r\nplt.xlabel('Boosting Iterations')\r\nplt.ylabel('Deviance')</code></pre></div>\n<p>위 사진의 왼쪽 그림을 만드는 코드이다. 의사결정나무를 몇개 만드냐에 따라 일탈의 정도가 어떻게 변하는지를 보여준다. 사진은 붉은 그래프가 계속 감소하지만 증가한다면 아마 오버피팅일테니 거기서 멈추면 되겠다.\r\n먼저 <code class=\"language-text\">clf.staged_precit(X_test)</code>는 의사결정나무 1개로 예측한 결과부터 500개까지 만들어 예측한 결과까지 다 담아 iteratable object로 리턴해준다. enumerate는 내용물을 꺼내면서 순서도 같이 꺼내주는 함수이다. 그래서 i에 인덱스를, y_pred에 데이터 자체를 꺼내어 test_score에 집어넣는 것이다.\r\n여기서 <code class=\"language-text\">clf.loss_()</code> 는 아까 ls를 적어줬으니 MSE를 사용할 것이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Plot feature importance\r\nfeature_importance = clf.feature_importances_\r\n# make importances relative to max importance\r\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\r\nsorted_idx = np.argsort(feature_importance)\r\npos = np.arange(sorted_idx.shape[0]) + .5\r\nplt.subplot(1, 2, 2)\r\nplt.barh(pos, feature_importance[sorted_idx], align='center')\r\nplt.yticks(pos, boston.feature_names[sorted_idx])\r\nplt.xlabel('Relative Importance')\r\nplt.title('Variable Importance')\r\nplt.show()</code></pre></div>\n<h1 id=\"feature-importances\" style=\"position:relative;\"><a href=\"#feature-importances\" aria-label=\"feature importances permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature Importances</h1>\n<p><code class=\"language-text\">clf.feature_importances_</code>는 각 열의 중요도를 반환해준다.</p>\n<h1 id=\"permutation-importances\" style=\"position:relative;\"><a href=\"#permutation-importances\" aria-label=\"permutation importances permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Permutation Importances</h1>\n<p>Feature Importances와 의미는 비슷하지만 계산방식이 다르다고.. 앵간해서는 후자가 더 좋다고함 번외로 LIME이라는 라이브러리도 비슷한 용도로 사용할 수 있다고 한다.</p>\n<p>원리는 각 열당 데이터를 무작위로 섞어서 무의미하게 만든다음 모델의 성능이 어떻게 나오는지 보는 것이다. 모델을 매번 수정하는건 비용이 크니 데이터를 수정해서 모델의 성능하락폭을 측정하는 것임.</p>\n<p><a href=\"https://soohee410.github.io/iml_permutation_importance\">https://soohee410.github.io/iml_permutation_importance</a></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from sklearn.inspection import permutation_importance\r\nfeature_importance = permutation_importance(clf, X_test, y_test, n_repeat=10, random_state=42, n_jobs=2)</code></pre></div>\n<p>n_repeat은 섞고 성능측정하는것을 반복하는 횟수이다. n_jobs는 멀티스레드 관련 파라미터.\r\n리턴방식은 2차원 array다.</p>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/49781727-1f4a-4597-9785-4181da61c903/image.png\" alt=\"\"></p>\n<p>각 행은 기존에 있던 속성을 의미, 열은 시도횟수를 뜻함 데이터는 기존 모델과 성능차이를 의미함. 이를 통해 boxplot 그림도 그릴 수 있다.</p>\n<p>절대적인 수치는 원래 성능과 특정 열을 뒤섞었을 때 metric의 차이를 의미한다.\r\nmetric은 <code class=\"language-text\">scoring</code> 이라고 하는 hyper-parameter에 따라 측정되는 지표이다.\r\n<code class=\"language-text\">scoring</code>을 따로 지정하지 않는 경우(위 코드와 같이) 사용되는 모델의 기본 scorer가 사용된다.\r\nscikit-learn의 모델들은 기본적으로 <code class=\"language-text\">model.score(x_test, y_test)</code>와 같이 score기능을 제공한다. 회귀분석의 경우 R2 Score(설명계수)를 활용하는데, 0~1사이에서 모델의 설명력을 평가한다. (높을수록 좋음) 분류분석의 경우 보통 Accuracy가 기본 scorer가 된다.</p>\n<h1 id=\"gradient-boosting-regression-실습분류\" style=\"position:relative;\"><a href=\"#gradient-boosting-regression-%EC%8B%A4%EC%8A%B5%EB%B6%84%EB%A5%98\" aria-label=\"gradient boosting regression 실습분류 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Gradient Boosting Regression 실습(분류)</h1>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn import datasets, ensemble\r\nfrom sklearn.model_selection import train_test_split\r\n# from sklearn.metrics import mean_squared_error\r\nfrom sklearn.metrics import accuracy_score</code></pre></div>\n<p>분류 문제이므로 mse 대신 accuracy score를 사용한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">cancer = datasets.load_breast_cancer()\r\nx, y = cancer.data, cancer.target \r\nx_train, y_train, x_test, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\r\n\r\nparams = {'n_estimators': 1000,\r\n\t\t'max_depth': 4,\r\n        'min_samples_split': 5,\r\n        'learning_rate': 0.01}\r\nclf = ensemble.GradientBoostingClassifier(**params)\r\nclf.fit(x_train, y_train)\r\n\r\nacc = accuracy_score(y_test, clf.predict(x_test))\r\nprint(acc)</code></pre></div>\n<p><code class=\"language-text\">GradientBoostingClassifier</code>를 사용한다는 점과 mse 대신 accuracy를 사용한다는 점이 다르다. 여기서 <code class=\"language-text\">clf.score(x, y)</code>를 사용해도 똑같이 accuracy가 나온다. 그래도 명시적으로 <code class=\"language-text\">accuracy_score</code>를 사용해주면 덜 헷갈린다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Plot training deviance\r\n\r\n# compute test set deviance\r\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\r\n\r\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\r\n    test_score[i] = accuracy_score(y_test, y_pred)\r\n\r\nplt.figure(figsize=(12, 6))\r\nplt.subplot(1, 1, 1)\r\nplt.title('Accuracy')\r\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\r\n         label='Training Set Deviance')\r\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\r\n         label='Test Set Deviance')\r\nplt.legend(loc='upper right')\r\nplt.xlabel('Boosting Iterations')\r\nplt.ylabel('Deviance')</code></pre></div>\n<p><code class=\"language-text\">test_score[i]</code> 를 <code class=\"language-text\">accuracy_score</code>로 사용하는 것이 바뀌었다.</p>\n<p>외에도 feature importances, roc_curve, auc 등 이전에 배운 것들을 사용할 수 있다.</p>\n<p>또 classification report라는 것이 있는데</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from sklearn.metrics import classification_report\r\n\r\npredictions = clf.predict(x_test)\r\n\r\nprint(classification_report(y_test, predictions))</code></pre></div>\n<p>이렇게 부르면</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">              precision    recall  f1-score   support\r\n\r\n     class 0       0.50      1.00      0.67         1\r\n     class 1       0.00      0.00      0.00         1\r\n     class 2       1.00      0.67      0.80         3\r\n\r\n    accuracy                           0.60         5\r\n   macro avg       0.50      0.56      0.49         5\r\nweighted avg       0.70      0.60      0.61         5</code></pre></div>\n<p><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html</a>\r\n요런 데이터가 나온다.</p>\n<p>precision은 각 class라고 예측한 것 들 중 맞아떨어진 비율\r\nrecall은 실제 class 인 것들 중 예측에 성공한 것 들의 비율\r\nf1-score는 이 둘의 조화평균\r\nsupport는 데이터의 개수\r\nmacro avg 는 각 열의 평균\r\nweighted avg는 support를 기준으로한 각 열의 가중평균</p>","tableOfContents":"<ul>\n<li><a href=\"#gradient-boosting-regression-%EC%8B%A4%EC%8A%B5%ED%9A%8C%EA%B7%80\">Gradient Boosting Regression 실습(회귀)</a></li>\n<li><a href=\"#feature-importances\">Feature Importances</a></li>\n<li><a href=\"#permutation-importances\">Permutation Importances</a></li>\n<li><a href=\"#gradient-boosting-regression-%EC%8B%A4%EC%8A%B5%EB%B6%84%EB%A5%98\">Gradient Boosting Regression 실습(분류)</a></li>\n</ul>","frontmatter":{"title":"AI/ML/Gradient Boosting","date":"November 22, 2022","description":null,"category":"AI"},"headings":[{"id":"gradient-boosting-regression-실습회귀"},{"id":"feature-importances"},{"id":"permutation-importances"},{"id":"gradient-boosting-regression-실습분류"}]},"previous":{"fields":{"slug":"/DecisionTree/"},"frontmatter":{"title":"AI/ML/Decision Tree"}},"next":{"fields":{"slug":"/연속된요소제거/"},"frontmatter":{"title":"String 연속된 요소 제거"}},"allMarkdownRemark":{"totalCount":22,"group":[{"fieldValue":"AI","totalCount":14},{"fieldValue":"Algorithm","totalCount":3},{"fieldValue":"Data Structure","totalCount":1},{"fieldValue":"Gatsby","totalCount":1},{"fieldValue":"JavaScript","totalCount":2},{"fieldValue":"Python","totalCount":1}]}},"pageContext":{"id":"211b08e7-7504-5979-82c7-7585c5feb82e","previousPostId":"db647d49-6a6b-5a71-9c3d-e8f1f1566db0","nextPostId":"27589e7d-1513-5e48-b9fd-3a611dd2488e"}},"staticQueryHashes":["2841359383"],"slicesMap":{}}