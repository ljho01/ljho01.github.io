{"componentChunkName":"component---src-templates-blog-post-js","path":"/SVM/","result":{"data":{"site":{"siteMetadata":{"title":"ljho01"}},"markdownRemark":{"id":"3e5798f5-89fd-558e-b623-4f8da446a556","excerpt":"Support Vector Machine Gradient Boosting(의사결정나무 기반 모델)과 같이 가장 강력한 머신러닝 모델 중 하나이다. 회귀, 분류 둘다 가능하다. 개념(분류 모델) https://static.javatpoint.com/tutorial/machine…","html":"<h1 id=\"support-vector-machine\" style=\"position:relative;\"><a href=\"#support-vector-machine\" aria-label=\"support vector machine permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Support Vector Machine</h1>\n<p>Gradient Boosting(의사결정나무 기반 모델)과 같이 가장 강력한 머신러닝 모델 중 하나이다. 회귀, 분류 둘다 가능하다.</p>\n<h2 id=\"개념분류-모델\" style=\"position:relative;\"><a href=\"#%EA%B0%9C%EB%85%90%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8\" aria-label=\"개념분류 모델 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>개념(분류 모델)</h2>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/19dff13f-a99a-4e48-87b0-4e8169240bbc/image.png\" alt=\"https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png\">\r\n클래스 간 margin을 최대화하는 결정경계 Decision Boundary를 찾는 것이 핵심이다. 경계에서 가까운 데이터를 Support vector라고 한다.</p>\n<p>positive plane과 negative plane 사이의 최소거리를 margin이라고 한다.</p>\n<p>positive plane, negative plane은 각각 support vector를 지난다</p>\n<h3 id=\"soft-margin-svm-hard-margin-svm\" style=\"position:relative;\"><a href=\"#soft-margin-svm-hard-margin-svm\" aria-label=\"soft margin svm hard margin svm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Soft Margin SVM, Hard Margin SVM</h3>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/03548725-c4fc-49e9-8ed3-5710f87f81e7/image.png\" alt=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\"></p>\n<p>경계를 넘는 데이터가 있으면 어떻게 Decision Boundary를 만들까? 기존 SVM의 목적함수에 약간의 오차를 허용하면 된다. 이를 Soft Margin SVM이라고 한다.(이전은 Hard Margin)</p>\n<p><a href=\"https://yngie-c.github.io/machine%20learning/2021/03/13/soft_margin_svm/\">https://yngie-c.github.io/machine%20learning/2021/03/13/soft_margin_svm/</a></p>\n<p>요기 c-svm의 목적 함수를 참고하면, w는 마진, xi는 일탈한 데이터와 p-plane, n-plane중 먼 것 과의 거리를 나타낸다. C는 가중치(Hyper-Parameter)다. 2/w인 margin과 달리 w^^2를 쓰는 이유는 미분하기 편해서 라고..</p>\n<p>이를 통해 모델의 Robustness가 올라간다. 이상치의 영향을 덜 받는다는 뜻이다.</p>\n<h3 id=\"kernel-svm\" style=\"position:relative;\"><a href=\"#kernel-svm\" aria-label=\"kernel svm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Kernel SVM</h3>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/288507f6-9c50-4678-a8d9-b8a9cfd2fb1d/image.png\" alt=\"Implementation and Analysis of Power Analysis Attack Using Multi-Layer\r\nPerceptron Method Hongpil Kwon, DaeHyeon Bae, Jaecheol Ha Hoseo University\"></p>\n<p>위와 같이 선형 분리 불가능(Linearly-Unseparable)한 데이터의 경계를 찾아내기 위해 Kernel SVM이 등장한다. 한 축에있는 데이터를 조작하여 새로운 z축을 만들어내고 입체화해서 결정 경계를 만든 후 다시 2차원으로 돌아오는 방식이다. 그러면 타원형 결정 경계가 생긴다.</p>\n<p>여기서 기존 데이터를 고차원으로 옮길 때 사용되는 함수를 커널 함수라고 한다. 이 커널 함수는 사람 마음대로 결정하는 것(Hyper-Parameter)이다. 물론 다항 커널, 가우시안 커널 등 이미 잘 만들어진 커널들이 많다. (<a href=\"https://sanghyu.tistory.com/14\">https://sanghyu.tistory.com/14</a>)</p>\n<p>기존 데이터를 조작해 새롭게 데이터를 만들어낸다는 점에서 딥러닝의 layer개념과 비슷하다. 근데 딥러닝에선 컴퓨터가 알아서 계산하지만 여기선 사람이 커널을 정해야 한다. 그래서 layer를 Learnable Kernel이라고도 한다. (<a href=\"https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture\">https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture</a>)</p>\n<h2 id=\"실습\" style=\"position:relative;\"><a href=\"#%EC%8B%A4%EC%8A%B5\" aria-label=\"실습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>실습</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from sklearn.svm import SVC\r\n\r\nclf = SVC(C=10, kernel='rbf, gamma=0.1)\r\nclf.fit(x, y)\r\n</code></pre></div>\n<p>보통 <code class=\"language-text\">SVC</code>, <code class=\"language-text\">SVR</code>을 많이 사용한다. <code class=\"language-text\">LinearSVC</code>같은 것도 있음.\r\nHyper-Parameter</p>\n<ul>\n<li>C: 목적함수에서 xi의 총합에 곱하는 계수. 클 수록 아웃라이어들에 민감해짐(오버피팅).</li>\n<li>kernel: 커널함수를 정할 수 있다. 'linear', 'poly', 'rbf' 등등 있는데 가우시안 커널 (Radial Basis Function)을 많이 사용한다고함</li>\n<li>gamma: 가우시안 커널에서 1/r. 커질수록 좀더 구체적이고 작은 타원모양이 나오고 작을 수록 둔하고 큰 타원모양이 나온다. (2차원에서)</li>\n</ul>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/2c736727-d834-4e6a-b3bb-9ad6c66492bc/image.png\" alt=\"https://bskyvision.com/entry/%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0SVM%EC%9D%98-%EC%82%AC%EC%9A%A9%EC%9E%90%EB%A1%9C%EC%84%9C-%EA%BC%AD-%EC%95%8C%EC%95%84%EC%95%BC%ED%95%A0-%EA%B2%83%EB%93%A4-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98-C%EC%99%80-gamma\"></p>\n<p>C와 gamma모두 커질수록 오버피팅되는 경향이 있다.</p>\n<p><strong>SVM은 X데이터들 간 스케일이 차이가 많이나면 성능이 잘 안나온다</strong>\r\n예를 들면 어떤 열은 10<del>100사이인데 어떤 열은 0.0001</del>0.01인 경우</p>\n<h2 id=\"feature-scaling\" style=\"position:relative;\"><a href=\"#feature-scaling\" aria-label=\"feature scaling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature Scaling</h2>\n<h3 id=\"min-max-normalization\" style=\"position:relative;\"><a href=\"#min-max-normalization\" aria-label=\"min max normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>min-max normalization</h3>\n<p>(x-min)/(max-min) 이렇게 0~1 사이에서 값을 조정한다.\r\nSVM뿐만 아니라 다른 모델에도 성능을 올려준다.\r\nTree기반 모델들은 그닥 필요하지 않다. 해도 상관은 없다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">sc = sklearn.preprocessing.MinMax~~~()\r\nsc.fit(x_train)</code></pre></div>\n<h3 id=\"standardization\" style=\"position:relative;\"><a href=\"#standardization\" aria-label=\"standardization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>standardization</h3>\n<p>평균이 0, 표준편차가 1이 되도록 표준화하는 것이다. (정규분포)</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">sc = sklearn.preprocessing.StandardScaler()\r\nsc.fit(x_train)\r\nx_train_scaled = sc.transform(x_train)\r\nx_test_scaled = sc.transform(x_test)\r\n\r\n...\r\n\r\nmodel.fit(x_test_scaled)\r\n</code></pre></div>\n<p>**스케일링 과정에서 test data가 개입하면 안된다.(평균, 최대, 최소, 표준편차 등) 쪼개고 Train data에만 하면 된다. **</p>\n<h2 id=\"hpo\" style=\"position:relative;\"><a href=\"#hpo\" aria-label=\"hpo permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>HPO</h2>\n<p>HPO(Hyper-Parameter-Optimization)</p>\n<ul>\n<li>Grid-SearchCV 직관적이고 성능도 좋음</li>\n<li>Randomized-Search 딥러닝에서 Grid-Search보다 성능이 잘나오는 경우가 있음.</li>\n<li>Bayesian-Search 난이도가 높음. 딥러닝에서 RandomizedSearch보다 잘 사용됨. AutoML 등에서 사용됨.</li>\n</ul>\n<p>보통 각 조합을 비교할 때도 Cross Validation을 사용하는데 이때 훈련 데이터를 또 쪼개서 검증하기 때문에 수동으로 각 조합을 비교한 것과 점수가 다르게 나올 수 있다. 그럴 땐 수동으로 비교해서 잘 나온 조합을 사용하면 된다.</p>\n<h3 id=\"gridsearchcv\" style=\"position:relative;\"><a href=\"#gridsearchcv\" aria-label=\"gridsearchcv permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GridSearchCV</h3>\n<p>HPO를 하기 위해 사용하는 모듈이다. (=HP Tuning, Model Tuning)\r\nHyper-Parameter의 후보군을 주면 알아서 조합해서 보여준다.\r\nCV는 Cross Validation의 줄임말이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from sklearn.model_selection import GridSearchCV\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.metrics import classification_report\r\n\r\nparam_grid = {'C':[1, 2, 3, 4, 5],\r\n\t\t\t'gamma':[0.1, 0.5, 1, 2, 100],\r\n            'kernel':['rbf']\r\n}\r\n\r\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\r\n# refit: 최적조합을 바로 적용할지 여부\r\n# verbose: print되는 설명의 정도\r\ngrid.fit(x_train_scaled, y_train)\r\n\r\n\r\ny_pred = grid.predict(x_test_scaled)\r\nprint(classification_report(y_test, y_pred))  </code></pre></div>\n<p><code class=\"language-text\">GridSearchCV()</code>에 모델이 들어갈 때 따로 argument를 주면 안되는데, <code class=\"language-text\">param_grid</code>에 추가해주는 식으로 대신할 수 있다.\r\n<code class=\"language-text\">grid.best_params_</code>로 최적해를 Dictionary로 꺼낼 수 있다.</p>\n<h3 id=\"randomizedsearchcv\" style=\"position:relative;\"><a href=\"#randomizedsearchcv\" aria-label=\"randomizedsearchcv permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>RandomizedSearchCV</h3>\n<p>Hyper-Parameter의 범위를 주면 조합해서 보여준다.</p>\n<h3 id=\"bayesiansearchcv\" style=\"position:relative;\"><a href=\"#bayesiansearchcv\" aria-label=\"bayesiansearchcv permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BayesianSearchCV</h3>\n<p>베이즈 통계학을 기반으로한 기법 (vs 빈도주의 통계학)</p>\n<p>참고할만한 것\r\n<a href=\"https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/\">https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/</a></p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#support-vector-machine\">Support Vector Machine</a></p>\n<ul>\n<li>\n<p><a href=\"#%EA%B0%9C%EB%85%90%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8\">개념(분류 모델)</a></p>\n<ul>\n<li><a href=\"#soft-margin-svm-hard-margin-svm\">Soft Margin SVM, Hard Margin SVM</a></li>\n<li><a href=\"#kernel-svm\">Kernel SVM</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EC%8B%A4%EC%8A%B5\">실습</a></p>\n</li>\n<li>\n<p><a href=\"#feature-scaling\">Feature Scaling</a></p>\n<ul>\n<li><a href=\"#min-max-normalization\">min-max normalization</a></li>\n<li><a href=\"#standardization\">standardization</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#hpo\">HPO</a></p>\n<ul>\n<li><a href=\"#gridsearchcv\">GridSearchCV</a></li>\n<li><a href=\"#randomizedsearchcv\">RandomizedSearchCV</a></li>\n<li><a href=\"#bayesiansearchcv\">BayesianSearchCV</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","frontmatter":{"title":"AI/ML/SVM(Support Vector Machine)","date":"December 07, 2022","description":null,"category":"AI"}},"previous":{"fields":{"slug":"/연속된요소제거/"},"frontmatter":{"title":"String 연속된 요소 제거"}},"next":{"fields":{"slug":"/진법/"},"frontmatter":{"title":"Algoritm/진법"}},"allMarkdownRemark":{"totalCount":22,"group":[{"fieldValue":"AI","totalCount":14},{"fieldValue":"Algorithm","totalCount":3},{"fieldValue":"Data Structure","totalCount":1},{"fieldValue":"Gatsby","totalCount":1},{"fieldValue":"JavaScript","totalCount":2},{"fieldValue":"Python","totalCount":1}]}},"pageContext":{"id":"3e5798f5-89fd-558e-b623-4f8da446a556","previousPostId":"27589e7d-1513-5e48-b9fd-3a611dd2488e","nextPostId":"4e479943-400a-59c5-8682-b426811f8928"}},"staticQueryHashes":["2841359383"],"slicesMap":{}}