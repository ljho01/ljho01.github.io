{"componentChunkName":"component---src-templates-blog-post-js","path":"/DecisionTree/","result":{"data":{"site":{"siteMetadata":{"title":"ljho01"}},"markdownRemark":{"id":"db647d49-6a6b-5a71-9c3d-e8f1f1566db0","excerpt":"의사결정나무 https://ratsgo.github.io/machine%20learning/2017/03/26/tree/ https://ratsgo.github.io/machine%20learning/2017/03/26/tree…","html":"<h1 id=\"의사결정나무\" style=\"position:relative;\"><a href=\"#%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4\" aria-label=\"의사결정나무 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>의사결정나무</h1>\n<p><img src=\"https://velog.velcdn.com/images/ljho01/post/64cd1574-a311-4bcf-bf8d-b2b5b0b0f027/image.png\" alt=\"https://ratsgo.github.io/machine%20learning/2017/03/26/tree/\"></p>\n<p><a href=\"https://ratsgo.github.io/machine%20learning/2017/03/26/tree/\">https://ratsgo.github.io/machine%20learning/2017/03/26/tree/</a> 이곳에서 더 자세히 알 수 있다. 각 변수들을 이곳저곳에 돌려 배치하면서 가장 명확한 트리를 찾는다.</p>\n<p>의사결정나무는 회귀, 분류 모두 해결 가능하다는 특징을 가진다.</p>\n<p>맨 위를 root node, 맨 밑을 leaf node라고 한다.\r\n아웃라이어들의 영향을 많이 받아 불안정하고, 과적합이 쉽게 발생한다.\r\n가지가 너무 길어지면 각 leaf node에 데이터가 부족해져서 신뢰하기가 좀 그렇다.</p>\n<h1 id=\"model-ensemble\" style=\"position:relative;\"><a href=\"#model-ensemble\" aria-label=\"model ensemble permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model ensemble</h1>\n<p>여러 모델을 짬뽕하여 예측하는 방법이다.</p>\n<ul>\n<li>Boosting</li>\n<li>Bagging</li>\n</ul>\n<p>등이 있다.</p>\n<p>여기서 의사결정나무의 단점을 보완하기 위해 Boosting기법을 사용한다. weak learner들을 합쳐 strong learner를 만드는 기법이다. 적당히 피팅된 의사결정나무(의사결정나무가 아니어도 됨) 여러개를 섞어 예측한다.</p>\n<p>분류문제의 경우 과반으로 예측을 하면되고, 회귀문제의 경우 평균/가중평균을 낸다.</p>\n<p>Tree-Based Ensemble Model들을 살펴보자</p>\n<h2 id=\"boosting\" style=\"position:relative;\"><a href=\"#boosting\" aria-label=\"boosting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Boosting</h2>\n<h3 id=\"adaboostingadaptive-boosting\" style=\"position:relative;\"><a href=\"#adaboostingadaptive-boosting\" aria-label=\"adaboostingadaptive boosting permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AdaBoosting(Adaptive Boosting)</h3>\n<p>Boosting기법의 업그레이드이다.\r\n<img src=\"https://velog.velcdn.com/images/ljho01/post/e43ff319-02bd-499f-8da9-9cd5925543bf/image.png\" alt=\"https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe\">\r\n<a href=\"https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe\">https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe</a></p>\n<ol>\n<li>데이터 학습</li>\n<li>틀린 부분에 가중치를 두고 재 학습 반복</li>\n<li>그 과정에있는 모델들을 모두 사용하여 결정</li>\n</ol>\n<p>위 사진의 경우 분류문제의 경우고 회귀문제의 경우 회귀를 많이해서 평균을 낸다.</p>\n<p>근데 가중치를 곱하다 보면 모델의 Acurracy가 점점 떨어져 쓰레기가 될 수 있다.\r\n어떻게 가중치를 줘야 하나? 여기서 Gradient decent가 들어온다. 가중치를 이리저리 조정해가면서 선정한다. 그렇게 Gradient Boosting 기법이 등장한다.</p>\n<p>Gradient Boosting을 사용해서 의사결정나무를 막 만들다보니 성능에 대한 아쉬움이 있었다. 의사결정 나무 자체가 만들기 오래걸리는 모델인데 Sequential로 만들다보니 너무 느리다. 그래서 등장한게 XG Boost이다.</p>\n<h3 id=\"xg-boost\" style=\"position:relative;\"><a href=\"#xg-boost\" aria-label=\"xg boost permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>XG Boost</h3>\n<p>병렬 처리 기법으로 Gradient Boosting의 성능을 끌어올렸다. Sequential하게 모델을 만드는 부분은 그대로 두되, 의사결정나무 자체를 만드는 과정에서 병렬 처리 기법을 사용하였다.</p>\n<p>XG Boost는 Hyper-Parameter가 꽤 많아서 최적의 설정값을 찾기가 까다롭다고 한다.</p>\n<p>번외로 MS의 LightGBM, EBM이라는 것도 있다.</p>\n<h2 id=\"bagging\" style=\"position:relative;\"><a href=\"#bagging\" aria-label=\"bagging permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Bagging</h2>\n<p>Bootstrap Aggregating의 줄임말이다.</p>\n<h3 id=\"random-forest\" style=\"position:relative;\"><a href=\"#random-forest\" aria-label=\"random forest permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Random Forest</h3>\n<p>가장 유명한 예는 Random Forest이다. Boosting과 같이 의사결정나무를 여러개 만들어서 합치는건 같으나 Boosting처럼 모델이 Sequential하게 만들어지지 않고 독립적으로 만들어진다.</p>\n<p>학습 데이터를 랜덤하게 뽑아서 모델을 만들기 때문이다. 병렬로 모델을 만들 수 있다. 그래서 성능이 좋지만 Boosting류가 성능이 일반적으로 좋다.</p>\n<h1 id=\"번외\" style=\"position:relative;\"><a href=\"#%EB%B2%88%EC%99%B8\" aria-label=\"번외 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>번외</h1>\n<p>n차원 테이블로 표현되는 정형데이터들은 머신러닝을, 이미지와 영상같은 데이터들은 딥러닝을 활용하는 경우가 많다.\r\n근데 정형데이터를 위한 인공신경망, 즉 딥러닝 모델인 TabNet이라는게 있다고..</p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"#%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4\">의사결정나무</a></p>\n</li>\n<li>\n<p><a href=\"#model-ensemble\">Model ensemble</a></p>\n<ul>\n<li>\n<p><a href=\"#boosting\">Boosting</a></p>\n<ul>\n<li><a href=\"#adaboostingadaptive-boosting\">AdaBoosting(Adaptive Boosting)</a></li>\n<li><a href=\"#xg-boost\">XG Boost</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#bagging\">Bagging</a></p>\n<ul>\n<li><a href=\"#random-forest\">Random Forest</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#%EB%B2%88%EC%99%B8\">번외</a></p>\n</li>\n</ul>","frontmatter":{"title":"AI/ML/Decision Tree","date":"November 21, 2022","description":null}},"previous":{"fields":{"slug":"/LogisticRegression/"},"frontmatter":{"title":"AI/ML/Logistic Regression"}},"next":{"fields":{"slug":"/GradientBoosting/"},"frontmatter":{"title":"AI/ML/Gradient Boosting"}}},"pageContext":{"id":"db647d49-6a6b-5a71-9c3d-e8f1f1566db0","previousPostId":"bd39c1b4-92fc-578d-b3e9-92f47200b86f","nextPostId":"211b08e7-7504-5979-82c7-7585c5feb82e"}},"staticQueryHashes":["2841359383"],"slicesMap":{}}